import torch
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
from torch.utils.data import Dataset, DataLoader
import yaml
import os
from src.models.ensemble import EnsembleVulnDetector
from src.data_pipeline.preprocessor import SecurityPreprocessor
from src.data_pipeline.tokenizer import MultiEmbeddingTokenizer

# Ø¯ÛŒØªØ§Ø³Øª ØªØ³Øª (Ù‡Ù…Ø§Ù† Ø³Ø§Ø®ØªØ§Ø± VulnDataset)
class TestVulnDataset(Dataset):
    def __init__(self, sec_bert_encodings, word2vec, fasttext, labels=None):
        self.sec_bert = sec_bert_encodings
        self.word2vec = word2vec
        self.fasttext = fasttext
        self.labels = labels  # Ù…ÛŒâ€ŒØªÙˆÙ†Ù‡ None Ø¨Ø§Ø´Ù‡ Ø§Ú¯Ø± unlabeled Ø¨Ø§Ø´Ù‡

    def __getitem__(self, idx):
        item = {k: v[idx] for k, v in self.sec_bert.items()}
        item['word2vec_embeds'] = torch.tensor(self.word2vec[idx])
        item['fasttext_embeds'] = torch.tensor(self.fasttext[idx])
        if self.labels is not None:
            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

    def __len__(self):
        return len(self.word2vec)


def load_model(config_path: str, model_path: str, device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    model = EnsembleVulnDetector(config)
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.to(device)
    model.eval()  # Ù…Ù‡Ù…: Ø­Ø§Ù„Øª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø±Ø§ÛŒ ÙØ¹Ø§Ù„ Ø´Ø¯Ù† Ø¢Ù†ÙˆÙ…Ø§Ù„ÛŒ
    print(f"âœ… loaded model in: {model_path}")
    print(f"   divice: {device}")
    return model, config


def test_model(
    model,
    config,
    csv_path: str,
    text_column: str = 'Sentence',
    label_column: str = 'label',  # ÛŒØ§ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ÛŒÙ†Ø±ÛŒ Ù…Ø«Ù„ SQLInjection, XSS, ...
    batch_size: int = 32,
    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
):
    """ØªØ³Øª Ú©Ø§Ù…Ù„ Ù…Ø¯Ù„ Ø±ÙˆÛŒ ÛŒÚ© Ø¯ÛŒØªØ§Ø³Øª CSV"""
    print("\n" + "="*60)
    print("ğŸš€ start test model...")
    print("="*60)

    # Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯ÛŒØªØ§Ø³Øª
    df = pd.read_csv(csv_path)
    print(f"count of sample: {len(df)}")

    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø§Ú¯Ø± Ø¨Ø§ÛŒÙ†Ø±ÛŒ Ø¨Ø§Ø´Ù‡
    if label_column not in df.columns:
        print("Ø³ØªÙˆÙ† label ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ â†’ ØªØ¨Ø¯ÛŒÙ„ Ø§Ø² Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ÛŒÙ†Ø±ÛŒ...")
        def get_label(row):
            if row.get('SQLInjection', 0) == 1: return 1
            elif row.get('XSS', 0) == 1: return 2
            elif row.get('CommandInjection', 0) == 1: return 3
            elif row.get('Normal', 0) == 1: return 0
            else: return -1
        df['label'] = df.apply(get_label, axis=1)
        df = df[df['label'] != -1].reset_index(drop=True)
        label_column = 'label'

    texts = df[text_column].tolist()
    labels = df[label_column].values if label_column in df.columns else None

    # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
    preprocessor = SecurityPreprocessor(config.get('preprocessing', {}))
    processed_texts = preprocessor.fit_transform(texts)

    # ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø±
    tokenizer = MultiEmbeddingTokenizer(config)
    tokenizer_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data', 'embeddings')
    tokenizer.load_word_embeddings(tokenizer_path)

    # Ø¬Ø§Ø³Ø§Ø²ÛŒâ€ŒÙ‡Ø§
    embeddings = tokenizer.encode(processed_texts)

    # Ø¯ÛŒØªØ§Ø³Øª Ùˆ Ø¯ÛŒØªØ§Ù„ÙˆØ¯Ø±
    dataset = TestVulnDataset(
        embeddings['sec_bert'],
        embeddings['word2vec'].numpy(),
        embeddings['fasttext'].numpy(),
        labels
    )
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)

    # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§
    all_preds = []
    all_labels = []
    all_anomaly_scores = []

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            word2vec = batch['word2vec_embeds'].to(device)
            fasttext = batch['fasttext_embeds'].to(device)

            outputs = model(input_ids, attention_mask, word2vec, fasttext)

            logits = outputs['logits']
            preds = torch.argmax(logits, dim=1).cpu().numpy()
            anomaly_scores = outputs['anomaly_scores'].cpu().numpy().flatten()

            all_preds.extend(preds)
            all_anomaly_scores.extend(anomaly_scores)
            if 'labels' in batch:
                all_labels.extend(batch['labels'].cpu().numpy())

    # Ú¯Ø²Ø§Ø±Ø´ Ù†ØªØ§ÛŒØ¬
    print("\n" + "="*60)
    print("ğŸ“Š Ù†ØªØ§ÛŒØ¬ ØªØ³Øª")
    print("="*60)

    if labels is not None:
        accuracy = accuracy_score(all_labels, all_preds)
        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)
        class_names = ['Normal', 'SQLi', 'XSS', 'CMDi']

        print(f"Ø¯Ù‚Øª Ú©Ù„ÛŒ (Accuracy): {accuracy:.4f}")
        print("\nreport for any class::")
        for i, name in enumerate(class_names):
            print(f"   {name:8} â†’ Precision: {precision[i]:.4f} | Recall: {recall[i]:.4f} | F1: {f1[i]:.4f}")

        print("\nClassification Report:")
        print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))

        print("\n(Confusion Matrix):")
        cm = confusion_matrix(all_labels, all_preds)
        print("       Pred â†’  Normal  SQLi   XSS   CMDi")
        for i, row in enumerate(cm):
            print(f"True {class_names[i]:6} â†’ {row}")

        # Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø´ØªØ¨Ø§Ù‡ Ø¨Ø§ ØªØ´Ø®ÛŒØµ Ø¢Ù†ÙˆÙ…Ø§Ù„ÛŒ
        print("\nğŸ” Û±Û° Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø´ØªØ¨Ø§Ù‡ (Ø¨Ø§ Ø§Ù…ØªÛŒØ§Ø² Ø¢Ù†ÙˆÙ…Ø§Ù„ÛŒ):")
        errors = np.where(np.array(all_preds) != np.array(all_labels))[0]
        for idx in errors[:10]:
            text = texts[idx]
            true = class_names[all_labels[idx]]
            pred = class_names[all_preds[idx]]
            anomaly = all_anomaly_scores[idx]
            print(f"   Ù…ØªÙ†: {text[:80]}{'...' if len(text)>80 else ''}")
            print(f"   true: {true} | predection: {pred} | anomaly: {anomaly:.2f}")
            print("   ---")
    else:
        print("Ø¯ÛŒØªØ§Ø³Øª Ø¨Ø¯ÙˆÙ† Ø¨Ø±Ú†Ø³Ø¨ â†’ ÙÙ‚Ø· Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
        for i in range(min(10, len(texts))):
            text = texts[i]
            pred = ['Normal', 'SQLi', 'XSS', 'CMDi'][all_preds[i]]
            anomaly = all_anomaly_scores[i]
            print(f"   Ù…ØªÙ†: {text[:80]}...")
            print(f"   predection: {pred} | score anomaly: {anomaly:.2f}")
            print("   ---")

    print(f"\n ave score anomaly in total dataset: {np.mean(all_anomaly_scores):.4f}")
    print("="*60)
    print("âœ… test finished...!")


# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø³Ø§Ø¯Ù‡
if __name__ == "__main__":
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    CONFIG_PATH = os.path.join(BASE_DIR, 'config', 'hyperparams.yaml')
    MODEL_PATH = os.path.join(BASE_DIR, 'final_model.pth')
    TEST_CSV_PATH = os.path.join(BASE_DIR, 'data', 'datasets', 'SQLInjection_XSS_CommandInjection_MixDataset.1.0.0.csv')  # Ù…Ø³ÛŒØ± Ø¯ÛŒØªØ§Ø³Øª ØªØ³Øª

    model, config = load_model(CONFIG_PATH, MODEL_PATH)

    test_model(
        model=model,
        config=config,
        csv_path=TEST_CSV_PATH,
        text_column='Sentence',
        label_column='label',
        batch_size=32
    )